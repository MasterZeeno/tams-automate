name: Automated Scraping

on:
  # Schedule the scraper to run daily at midnight (UTC)
  schedule:
    - cron: "0 0 * * *"
  # Optionally, run it when a push is made to the main branch
  push:
    branches:
      - main

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      # Step 1: Checkout the repository
      - name: Checkout code
        uses: actions/checkout@v3

      # Install necessary dependencies for Puppeteer
      - name: Install Puppeteer dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libnss3 libatk-bridge2.0-0 libxshmfence1 libgbm1 libasound2 libxrandr2 libxss1 libgtk-3-0

      # Step 2: Set up Node.js (use your projectâ€™s version)
      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: "18.x"

      # Step 3: Install dependencies
      - name: Install dependencies
        run: npm install

      # Step 4: Set up environment variables (replace these with your actual variables)
      - name: Set environment variables
        run: |
          echo "ZEE_USERNAME=${{ secrets.ZEE_USERNAME }}" >> $GITHUB_ENV
          echo "ZEE_PASSWORD=${{ secrets.ZEE_PASSWORD }}" >> $GITHUB_ENV
          echo "TAMS_BASE_URL=${{ secrets.TAMS_BASE_URL }}" >> $GITHUB_ENV
          echo "HCC_BASE_URL=${{ secrets.HCC_BASE_URL }}" >> $GITHUB_ENV

      # Step 5: Run the scraping script
      - name: Run scraper
        run: DEBUG="puppeteer:*" node scrapper3.js

      # Optional Step 6: Commit and push results back to the repository (if needed)
      - name: Commit and Push results
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add results/*.json
          git commit -m "Scraped data from $(date +"%Y-%m-%d %H:%M:%S")"
          git push
